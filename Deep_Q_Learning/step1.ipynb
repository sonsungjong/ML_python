{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbe569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "## !apt-get install -y swig    # 직접 설치하고 환경변수 등록\n",
    "# !pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd41acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3f293",
   "metadata": {},
   "source": [
    "Part 1 - Building the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3860f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed = 42):\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)    # 첫번째 층과 동일한 두번째 층 레이어\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "def forward(self, state):\n",
    "    x = self.fc1(state)\n",
    "    x = F.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = F.relu(x)\n",
    "    return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b179ae",
   "metadata": {},
   "source": [
    "Part2 - Training the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0b7df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "State size:  8\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "# Setting up the environment\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "number_actions = env.action_space.n\n",
    "print('State shape: ', state_shape)\n",
    "print('State size: ', state_size)\n",
    "print('Number of actions:', number_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e2bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the hyperparameters\n",
    "learning_rate = 5e-4\n",
    "minibatch_size = 100\n",
    "discount_factor = 0.99\n",
    "replay_buffer_size = int(1e5)\n",
    "interpolation_parameter = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccf2f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Experience Replay\n",
    "class ReplayMemory(object):\n",
    "    # 생성자에서 CUDA 사용이 가능하면 GPU를, 불가능하면 CPU를 사용하게 한다\n",
    "    def __init__(self, capacity):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        return states, next_states, actions, rewards, dones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2f0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the DQN class\n",
    "class Agent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "        self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n",
    "        self.memory = ReplayMemory(replay_buffer_size)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    # 경험을 저장\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory.memory) > minibatch_size:\n",
    "                experiences = self.memory.sample(100)\n",
    "                self.learn(experiences, discount_factor)\n",
    "\n",
    "    def act(self, state, epsilon=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.local_qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.local_qnetwork(state)\n",
    "        self.local_qnetwork.train()         # 훈련모드로 돌입\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, discount_factor):\n",
    "        states, next_states, actions, rewards, dones = experiences\n",
    "        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + (discount_factor * next_q_targets * (1-dones))\n",
    "        q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02c384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb68c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ee689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58908e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f260a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7f03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e94894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb8d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3cad6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
