{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General information\n",
    "\n",
    "이 내용은 안드레이 룩야넨코의 노트북을 한글화한 것 입니다.\n",
    "이 커널에서는 IEEE Fraud Detection competition을 다룹니다.\n",
    "\n",
    "IEEE-CIS는 다양한 분야의 AI 및 머신러닝 부분에서 일하고 있는데요\t\n",
    "이는 deep neural networks, fuzzy systems, evolutionary computation 및 swarm intelligence를 포함합니다\n",
    "이제 세계 굴지의 지불 시스템 회사인 Vesta와 협력하여, 카드 사기 방지를 할 수 있는 최적의 솔루션을 찾아서 수 많은 데이터 사이언티스트에게 이에 도전하라고 했습니다\n",
    "\n",
    "이 챌린지는 binary classification problem으로 이러한 과제가 경험할 수 밖에 없는 data의 심한 imbalance를 보여 줍니다.\n",
    "우리는 여기서 데이터를 탐구하고 소중한 인사이트를 파악하여 많은 feature engineering을 통하여 성능이 높은 모델링을 하려고 합니다\n",
    "\n",
    "![](https://cis.ieee.org/images/files/slideshow/abstract01.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.precision = 15\n",
    "pd.options.display.max_rows = 10000\n",
    "pd.options.display.max_columns = 10000\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "from numba import jit\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "데이터를 우선 로딩하고 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/train_identity.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16164\\3072927152.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../input/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_identity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{folder_path}train_identity.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_transaction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{folder_path}train_transaction.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_identity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{folder_path}test_identity.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_transaction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{folder_path}test_transaction.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/train_identity.csv'"
     ]
    }
   ],
   "source": [
    "folder_path = '../input/'\n",
    "train_identity = pd.read_csv(f'{folder_path}train_identity.csv')\n",
    "train_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\n",
    "test_identity = pd.read_csv(f'{folder_path}test_identity.csv')\n",
    "test_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\n",
    "sub = pd.read_csv(f'{folder_path}sample_submission.csv')\n",
    "# let's combine the data and work with the whole dataset\n",
    "train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        구글 콜랩을 사용하시면 데이터를 PC에 다운로드 하신 후 그 파일들을 아래 코드를 활용하여 다시 업로드 함으로 import 하세요 \n",
    "\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "\n",
    "        이 다음에 파일 선택하고 (미리 다운로드 받아 놓으셔야 해요) 아래 코드로 변환 시키면 됩니다.\n",
    "\n",
    "        그런데 콜래벵서 업로드 하면 시간이 엄청 걸립니다. (콜랩에서는 고용량 RAM 체크하고 하셔야 합니다)\n",
    "        \n",
    "        이어서 아래 실행\n",
    "\n",
    "        import io\n",
    "        train_identity = pd.read_csv(io.BytesIO(uploaded['train_identity.csv']))\n",
    "        train_transaction = pd.read_csv(io.BytesIO(uploaded['train_transaction.csv']))\n",
    "        test_identity = pd.read_csv(io.BytesIO(uploaded['test_identity.csv']))\n",
    "        test_transaction = pd.read_csv(io.BytesIO(uploaded['test_transaction.csv']))\n",
    "        sub = pd.read_csv(io.BytesIO(uploaded['sample_submission.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 열이 많은 꽤 큰 데이터셋 두 개를 볼 수 있습니다. Train과 test data는 비슷한 길이의 행을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_identity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transaction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터는 두 개로 나누어져 있는 것 같습니다: identity와 transaction.\n",
    "\n",
    "아무래도 identity는 어떤 카드인가에 초점이 맞추어 지겠고 id 30또는 31을 보면 안드로이드나 크롬 같은 단어도 나옵니다?\n",
    "\n",
    "Identity 40개 가량의 열이 있고 열의 내용을 눈치채게 할 이름은 없습니다.\n",
    "\n",
    "하지만 사용자의 버전을 볼 수 있죠: OS, browser version, resolution, type 및 기타 등 등 \n",
    "\n",
    "Transaction data는 거의 400 columns가 있습니다!\n",
    "\n",
    "주목 할만한 열들은 다음과 같습니다:\n",
    "\n",
    "card information\n",
    "\n",
    "transaction date & amount\n",
    "\n",
    "e-mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터를 합쳐 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떤 이유에선지 테스트 컬럼에서 언더바 대신 하이픈으로 쓴 것이 있어서 좀 바꿉니다.\n",
    "test_identity.columns = [col.replace('-', '_') for col in test_identity.columns]\n",
    "\n",
    "# merge method를 사용하여 트레인과 테스트의 트랜잭션(거래)와 아이덴티티(신분)을 파일을 합합니다. \n",
    "# 여기서 on은 그 것을 인덱스로 합치라는 것이고 left나 right는 왼쪽 것 기준으로 또는 오른쪽 것 기준으로 파일을 정열하는데...여기서 말하는 기준은 데이터 값이 있는 것입니다.\n",
    "# 다시 말해 left로 되어 있는데 왼쪽의 transaction의 해당 행이 빈 값이면 identity에 값이 있어도 그 행은 보이지 않게되고, 반대로 right라면 그 행은 보이게 됩니다.\n",
    "\n",
    "train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 기존 데이터는 무거우니 지웁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_transaction, train_identity, test_transaction, test_identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트레인 데이터에는 C1부터 C15까지 빈 값이 없으나 테스트 데이터에서는 빈값이 있어서 이들의 빈 값을 0으로 채웁니다.\n",
    "\n",
    "아래에서 f는 f-string이라 불리는 것으로 이를 통해 다양한 표현식을 사용할 수 있다.\n",
    "\n",
    "예를 들어 python2에서 나오는 %나 python3에서 나오는 str.format과 비교하였을 때 훨씬 간단하게 표현할 수 있다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[[f'C{i}' for i in range(1,15)]] = test[[f'C{i}' for i in range(1,15)]].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이어서 얼마나 많은 램을 쓰는지 한 번 보기로 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_mem = train.memory_usage().sum() / 1024 ** 2 + test.memory_usage().sum() / 1024 ** 2\n",
    "print(f'Mem. usage {end_mem:5.2f} Mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 상당히 많이 쓴 것으로 RAM사용량을 줄이는 메소드가 있으므로 사용해 보도록 합니다\n",
    "\n",
    "정보는 그대로 주고 데이터 타입을 줄이므로 이를 달성합니다.\n",
    "\n",
    "아시다시피 integer가 float보다 메모리를 덜 사용하고 integer8이 integer16보다 메모리를 덜 사용합니다.\n",
    "\n",
    "일단 낮출 수 잇는 만큼 낮추어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(memory_df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = memory_df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in memory_df.columns:\n",
    "        col_type = memory_df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = memory_df[col].min()\n",
    "            c_max = memory_df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    memory_df[col] = memory_df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    memory_df[col] = memory_df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    memory_df[col] = memory_df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    memory_df[col] = memory_df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    memory_df[col] = memory_df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    memory_df[col] = memory_df[col].astype(np.float32)\n",
    "                else:\n",
    "                    memory_df[col] = memory_df[col].astype(np.float64)\n",
    "    end_mem = memory_df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
    "            start_mem - end_mem) / start_mem))\n",
    "    return memory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메모리 사용도가 2/3가 줄어 1/3 수준으로 줄어들었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "이제 identity information을 파악해 봅니다.\n",
    "\n",
    "id_01 - id_11는 continuous variables입니다.\n",
    "\n",
    "id_12 - id_38는 categorical variable이고 마지막 두 개의 열은 확실히 범주 항목입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train['id_01'], bins=77);\n",
    "plt.title('Distribution of id_01 variable');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`id_01` 은 아주 흥미로운 분포를 가지고 있습니다. 77개의 유니크한  양수가 아닌 수로 0에서 비대칭이 일어나는 분포입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['id_03'].value_counts(dropna=False, normalize=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`id_03` 은 88%가 빈 값이며 98%가 빈값이거나 0입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['id_11'].value_counts(dropna=False, normalize=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`id_11` 의 값 중 22%는 100이며 76%는 빈 값입니다. 좀 이상합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train['id_07']);\n",
    "plt.title('Distribution of id_07 variable');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일부 항목은 normalized 된 것 같습니다. 모든 항목을 normalize하려면 이미 normalized 된 것은 분리하여야 할 것입니다. \n",
    "\n",
    "이제 transaction data를 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.hist(train['TransactionDT'], label='train');\n",
    "plt.hist(test['TransactionDT'], label='test');\n",
    "plt.legend();\n",
    "plt.title('Distribution of transactiond dates');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 중요점이 발견 됩니다.\n",
    "\n",
    "트레인 및 테스트 트랜잭션 날짜가 겹치지 않는 것 같으므로 유효성 검사를 위해 시간 기반 분할을 사용하는 것이 현명할 것입니다.\n",
    "\n",
    "대회 설명에서도 다음과 같은 내용이 있었습니다. \"TransactionDT\" 항목은 주어진 참조 날짜 / 시간 (실제 타임 스탬프 아님)의 타임 델타입니다.\n",
    "\n",
    "시작일이 2017-11-30이라는 것을 깨달았습니다. 이 정보는 나중에 사용할 것입니다.\n",
    "\n",
    "* 참조 사항\n",
    "\n",
    "우리가 여기서 사기를 일으키는 카드를 찾는 것이 아니라 그런 사용자를 찾는 것이란 것을 명심하세요.\n",
    "\n",
    "위의 TransactionDT를 설명 드리면 거래를 하려는데 카드가 정지되거나 카드를 누가 불법으로 썼는데 카드 주인이 모르고 넘어 간다거나 등 등에서 모두 날짜가 중요하겠죠. \n",
    "\n",
    "이에 대한 reference 포인트가 될 항목입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "start_date = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values in `card` columns\n",
    "\n",
    "Сard columns are important but missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if 'card' in col:\n",
    "        print(f\"{col} has {train[col].isnull().sum()} missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'card1'의 각 값에 대한 'card2'의 가장 일반적인 값을 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_with_mode = train.groupby(['card1']).agg({'card2': ['nunique', pd.Series.mode]})\n",
    "group_with_mode.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'card1'의 유니크한 값이 얼마나 있는지, 1개를 초과하는 유니크한 값을 가진 'card1'의 유니크한 값이 얼마나 있는지 보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_one = group_with_mode[group_with_mode['card2']['nunique'] > 1].shape[0]\n",
    "card1_nunique = train['card1'].nunique()\n",
    "print(f'Number of unique values of card1: {card1_nunique}')\n",
    "print(f'Number of unique values of card1 which have more than one unique value: {non_one}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 경우 card1의 각 값에 대해 card2 열에는 고유 한 값이 하나만 있습니다. 이것을 참조하여 누락 된 값을 채워 봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for card in ['card2','card3','card4','card5','card6']:\n",
    "    group_with_mode = train.groupby(['card1']).agg({card: ['nunique', pd.Series.mode]})\n",
    "    to_merge = group_with_mode[group_with_mode[card]['nunique'] == 1][card]['mode']\n",
    "    merged = pd.merge(train['card1'], to_merge, on='card1', how='left')\n",
    "    merged['mode'] = merged['mode'].fillna(train[card])\n",
    "    train[card] = merged['mode']\n",
    "    \n",
    "    group_with_mode = test.groupby(['card1']).agg({card: ['nunique', pd.Series.mode]})\n",
    "    to_merge = group_with_mode[group_with_mode[card]['nunique'] == 1][card]['mode']\n",
    "    merged = pd.merge(test['card1'], to_merge, on='card1', how='left')\n",
    "    merged['mode'] = merged['mode'].fillna(test[card])\n",
    "    test[card] = merged['mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target and Users\n",
    "\n",
    "라벨링 로직에 대한 정보는 https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#588953 에서 볼 수 있습니다.\n",
    "```\n",
    "라벨링의 논리는 카드에 보고된 지불 거절을 사기 거래 (isFraud = 1)로 정의하고 사용자 계정, 이메일 주소 또는 이러한 속성에 직접 연결된 청구 주소를 사기로 사후에 거래하는 것입니다. \n",
    "\n",
    "120 일이 지난 후에도 위에 나온 것들에서 보고된 것이 없는 경우 합법적인 거래로 정의합니다 (isFraud = 0).\n",
    "```\n",
    "\n",
    "결과적으로 사기의 경우 단일 거래가 사기로 간주될 뿐만 아니라 사용자의 다른 거래도 사기로 간주된다는 것을 알 수 있기 때문에 이것은 매우 중요합니다. (다른 카드 모두 정지될 테니까요)\n",
    "\n",
    "하지만 우리에게는 데이터에 사용자 ID가 있는 열이 없습니다. 익명 처리되었습니다. \n",
    "\n",
    "결과적으로 다른 열을 기반으로 사용자 ID 열을 만들어야 겠지요. \n",
    "\n",
    "예를 들어 주소와 카드 정보의 조합을 ID로 사용할 수 있습니다.\n",
    "\n",
    "우리는 'D1'이 사용자의 첫 거래 이후 일수를 의미한다는 것을 알고 있습니다.\n",
    "\n",
    "더 나아가기 전에 캐글의 컬럼 정보 부분을 다시 보겠습니다.\n",
    "\n",
    "* TransactionDT: 주어진 포인트로부터 타임델타 (실 타임 스템프는 아님)\n",
    "* TransactionAMT: 미국 달러로 결제한 거래 액수\n",
    "* ProductCD: 상품 코드, 거래한 상품\n",
    "* card1 — card6: 지불 수단 카드 정보, 카드 타입, 카드 분류, 은행, 국가 등\n",
    "* addr: 주소\n",
    "* dist: 거리\n",
    "* P_ and (R__) emaildomain: 구매자의 이메일 정보\n",
    "* C1-C14: 카운팅 얼마나 많은 주소가 연결되었느냐 등인데 실제 내용은 마스킹 되어 있어서 파악이 안 됨\n",
    "* D1-D15: 타임델타, 예를 들어 그 전 거래부터 얼마나 되었는지 등\n",
    "* M1-M9: 매치...카드와 이름 주소의 매치 등\n",
    "* Vxxx: Vesta 제작 항목 랭킹, 카운팅, 기타 관련\n",
    "* identity table의 항목은 네트워크 연결 정보 예를 들어 IP, ISP, Proxy, 등 또는 디지털 시그내쳐 UA/browser/os/version 등으로 거래 관련 정보를 나타낸다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DaysFromStart라는 항목을 만듭니다. \n",
    "\n",
    "TransactionDT에서 60초 60분 24시간을 나누어서 버림(floor)로 해서 1을 뺀 날짜를 구하고 \n",
    "\n",
    "D1-DaysFromStart 항목을 만들어 D1-DaysFromStart 날짜를 봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train,test]:\n",
    "    df['DaysFromStart'] = np.floor(df['TransactionDT']/(60*60*24)) - 1\n",
    "    df['D1-DaysFromStart'] = df['D1'] - df['DaysFromStart']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이어서 연속으로 정보를 합쳐서 uid를 생성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train,test]:\n",
    "    df['uid'] = df['ProductCD'].astype(str) + '_' + df['card1'].astype(str) + '_' + df['card2'].astype(str)\n",
    "    df['uid'] = df['uid'] + '_' + df['card3'].astype(str) + '_' + df['card4'].astype(str)\n",
    "    df['uid'] = df['uid'] + '_' + df['card5'].astype(str) + '_' + df['card6'].astype(str)\n",
    "    df['uid'] = df['uid'] + '_' + df['addr1'].astype(str) + '_' + df['D1-DaysFromStart'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uid의 크기와 유니크한 값을 보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train['uid']).intersection(set(test['uid']))), train.uid.nunique(), test.uid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['uid'].value_counts(dropna=False, normalize=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트레인 데이터와 테스트 데이터간에 일부 uid 만 겹치는 것을 볼 수 있습니다. 즉, 이 항목을 직접 적용하여 사용할 수 없으며 이를 기반으로 다양한 항목을 만들어 사용해야 할 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing some device information and ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeviceInfo의 밸류 카운트를 보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['DeviceInfo'].value_counts(dropna=False, normalize=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보시면, 거의 80%가 빈 값이고. 윈도우가 8%가량, iOS가 3.34% 등 나오고 삼성도 좀 보입니다.\n",
    "\n",
    "유니크한 값이 몇개 인지 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['DeviceInfo'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 많은 디바이스가 있습니다. 그리고 80% 가량은 빈값이니 어느 장비인 줄 알 수가 없습니다.\n",
    "\n",
    "이를 좀 고쳐서 항목을 만들어 보려 하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join은 데이터를 합쳐 주는데 dtype이 같아야만 합니다\n",
    "\n",
    "모델명 숫자 등을 지우고 종류를 정리해 보았습니다\n",
    "\n",
    "아래에서 얼마나 줄어들었는지 봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DeviceInfo_device'].value_counts(dropna=False, normalize=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DeviceInfo_version'].value_counts(dropna=False, normalize=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'id_30' 의 밸류 카운트를 비율로 봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id_30'].value_counts(dropna=False, normalize=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유니크한 값이 몇개나 있는지 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['id_30'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'id_31' 의 밸류 카운트를 비율로 봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id_31'].value_counts(dropna=False, normalize=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이들을 위에 DeviceInfo에 했던 것 처럼 해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n",
    "    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "\n",
    "    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n",
    "    df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빈 값에는 unknown device라 입력을 하고 합치고 줄여 봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id_30_device'].value_counts(dropna=False, normalize=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process some D-columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 Card 열들은 카드 종류나 카드사 같은 정보라는 것을 위에서 보았으며\n",
    "\n",
    "id는 사용된 OS나 디바이스 등임을 알았습니다.\n",
    "\n",
    "이제 D항목들은 어떤 것인지 보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 D1 에서 D15까지 어떤 값들이 있나 봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D4.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D5.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D6.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D7.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D8.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D9.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D10.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D11.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D12.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D13.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D14.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.D15.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D8과 D9의 분포부터 보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train['D8']);\n",
    "plt.title('Distribution of D8 variable');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train['D9']);\n",
    "plt.title('Distribution of D9 variable');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    # 클립 메소드를 사용하면 음수를 0으로 만들 수 있습니다\n",
    "    for col in ['D'+str(i) for i in range(1,16)]:\n",
    "        df[col] = df[col].clip(0)\n",
    "    \n",
    "    # D9을 빈 값이 아닌 것을 따로 떼어내어 D9_not_na 항목을 새로 만듭니다.\n",
    "    df['D9_not_na'] = np.where(df['D9'].isna(),0,1)\n",
    "    # D8을 1보다 크거나 같은 것만 떼어내어 D8_not_same_day 항목을 만듭니다\n",
    "    df['D8_not_same_day'] = np.where(df['D8']>=1,1,0)\n",
    "    # D8에서 소수점 이하를 뽑아서 D8_D9_decimal_dist로 정의한 후 여기서 D9을 뺍니다\n",
    "    df['D8_D9_decimal_dist'] = df['D8'].fillna(0)-df['D8'].fillna(0).astype(int)\n",
    "    df['D8_D9_decimal_dist'] = ((df['D8_D9_decimal_dist']-df['D9'])**2)**0.5\n",
    "    df['D8'] = df['D8'].fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.D9_not_na.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.D8_not_same_day.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.D8_D9_decimal_dist.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.D8.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process `TransactionAmt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제는 거래 액수에 대해 보겠습니다 \n",
    "\n",
    "분포를 보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train['TransactionAmt']);\n",
    "plt.title('Distribution of TransactionAmt variable');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 1%(퍼센타일 99) 를 봅니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(train['TransactionAmt'], 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클립하여 0부터 5000사이의 값으로 봅니다\n",
    "train['TransactionAmt'] = train['TransactionAmt'].clip(0,5000)\n",
    "test['TransactionAmt']  = test['TransactionAmt'].clip(0,5000)\n",
    "\n",
    "# 거래 액수가 일반적인지 아닌지를 봅니다\n",
    "train['TransactionAmt_check'] = np.where(train['TransactionAmt'].isin(test['TransactionAmt']), 1, 0)\n",
    "test['TransactionAmt_check']  = np.where(test['TransactionAmt'].isin(train['TransactionAmt']), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TransactionAmt_check.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.TransactionAmt_check.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트레인 거래액수에서 테스트 거래 액수에 없는 것이 16,920건 테스트 거래 액수에서 트레인 거래 액수에 없는 것이 2363건인 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 유용한 항목들을 만들어 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df['ProductCD_card1'] = df['ProductCD'].astype(str) + '_' + df['card1'].astype(str)\n",
    "    df['card1_addr1'] = df['card1'].astype(str) + '_' + df['addr1'].astype(str)\n",
    "    df['TransactionAmt_dist2'] = df['TransactionAmt'].astype(str) + '_' + df['dist2'].astype(str)\n",
    "    df['card3_card5'] = df['card3'].astype(str) + '_' + df['card5'].astype(str)\n",
    "    df['ProductCD_TransactionAmt'] = df['ProductCD'].astype(str) + '_' + df['TransactionAmt'].astype(str)\n",
    "    df['cents'] = np.round(df['TransactionAmt'] - np.floor(df['TransactionAmt']), 3)\n",
    "    df['ProductCD_cents'] = df['ProductCD'].astype(str) + '_' + df['cents'].astype(str)\n",
    "    df['TransactionAmt'] = np.log1p(df['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProductCD_card1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.card1_addr1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TransactionAmt_dist2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProductCD_TransactionAmt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TransactionAmt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ProductCD_card1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 항목들은 다른 것을 만드는데만 유용하니 이제 없애도록 하겠습니다\n",
    "train = train.drop(['DaysFromStart','D1-DaysFromStart'], axis=1)\n",
    "test = test.drop(['DaysFromStart','D1-DaysFromStart'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 날짜 기반 항목들을 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    # 집계를위한 임시 항목들\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x: (start_date + datetime.timedelta(seconds=x)))\n",
    "    df['DT_M'] = ((df['DT'].dt.year - 2017) * 12 + df['DT'].dt.month).astype(np.int8)\n",
    "    df['DT_W'] = ((df['DT'].dt.year - 2017) * 52 + df['DT'].dt.weekofyear).astype(np.int8)\n",
    "    df['DT_D'] = ((df['DT'].dt.year - 2017) * 365 + df['DT'].dt.dayofyear).astype(np.int16)\n",
    "\n",
    "    df['DT_hour'] = df['DT'].dt.hour.astype(np.int8)\n",
    "    df['DT_day_week'] = df['DT'].dt.dayofweek.astype(np.int8)\n",
    "    df['DT_day_month'] = df['DT'].dt.day.astype(np.int8)\n",
    "\n",
    "    # 잠재적 솔로 항목\n",
    "    df['is_december'] = df['DT'].dt.month\n",
    "    df['is_december'] = (df['is_december'] == 12).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "올바른 유효성 검사는 매우 중요합니다. \n",
    "\n",
    "로컬 점수가 리더 보드 점수와 관련이 있는 경우 모델을 로컬에서 학습 및 비교하고 때때로 제출할 수 있습니다. \n",
    "\n",
    "로컬 점수가 리더 보드 점수와 관련이 없는 경우 모델이 좋은지 여부를 실제로 추정 할 수 없습니다.\n",
    "\n",
    "데이터에 날짜가 있으므로 일종의 시계열 검증을 사용해야합니다. \n",
    "\n",
    "즉, 훈련 데이터가 항상 검증 데이터보다 시간 상으로 먼저 있어야합니다.\n",
    "\n",
    "이 커널에서는 학습을 위해 처음 80 % 데이터를, 유효성 검사를 위해 마지막 20%를 취하겠습니다. \n",
    "\n",
    "신뢰성을 보장하기 위해 다양한 랜덤 시드로 여러 모델을 훈련시키고 점수를 평균화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['isFraud']\n",
    "X = train.drop(['isFraud'], axis=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the data separately\n",
    "\n",
    "검증이 올바른지 확인하기 위해 트레인 및 검증 데이터를 트레인 및 테스트 데이터를 다룰 때와 동일하게 처리해야합니다.\n",
    "\n",
    "이를 위해 두 개의 데이터 프레임을 처리하는 함수를 작성합니다.\n",
    "\n",
    "이 함수에서 모델링에 사용되지 않을 항목을 위해 remove_features 목록을 만들 것입니다.\n",
    "\n",
    "그들 중 일부는 일시적인 것이었고 일부는 쓸모 없거나 나쁜 것으로 판명되었습니다.\n",
    "\n",
    "먼저 프리퀀시 인코딩을 위한 함수를 작성해 보겠습니다.\n",
    "\n",
    "두 개의 데이터 프레임을 사용하고 이러한 데이터 프레임에서 하나의 열을 결합하고 그 안에있는 각 범주의 수를 계산합니다.\n",
    "\n",
    "카테고리 대신 이러한 값을 사용하여 새 열을 만드는 데 사용됩니다.\n",
    "\n",
    "이것은 널리 사용되는 kaggle 기술입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_encode_full(df1, df2, col, normalize=True):\n",
    "    \"\"\"\n",
    "    Encode\n",
    "\n",
    "    https://www.kaggle.com/cdeotte/high-scoring-lgbm-malware-0-702-0-775\n",
    "    \"\"\"\n",
    "    df = pd.concat([df1[col], df2[col]])\n",
    "    freq_dict = df.value_counts(dropna=False, normalize=normalize).to_dict()\n",
    "    col_name = col + '_freq_enc_full'\n",
    "    return col_name, freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df_train: pd.DataFrame, df_test: pd.DataFrame):\n",
    "    # 제자리에서 변경되지 않도록 복사하십시오.\n",
    "    train_df = df_train.copy()\n",
    "    test_df = df_test.copy()\n",
    "    remove_features = ['TransactionID', 'TransactionDT', 'DT', 'DT_M', 'DT_W', 'DT_D', 'DT_hour', 'DT_day_week',\n",
    "                       'DT_day_month',\n",
    "                       'ProductCD_card1', 'card1_addr1', 'TransactionAmt_dist2', 'card3_card5', 'uid',\n",
    "                       'D5_DT_W_std_score',\n",
    "                       'ProductCD_TransactionAmt_DT_W', 'D4_DT_D_std_score', 'D15_DT_D_std_score', 'D3_DT_W_std_score',\n",
    "                       'D11_DT_W_std_score',\n",
    "                       'card3_card5_DT_W_week_day_dist', 'card5_DT_W_week_day_dist', 'D10_DT_D_std_score',\n",
    "                       'card3_card5_DT_D', 'ProductCD_cents_DT_D',\n",
    "                       'D4_DT_W_std_score', 'D15_DT_W_std_score', 'uid_DT_D', 'card3_DT_W_week_day_dist',\n",
    "                       'D10_DT_W_std_score', 'D8_DT_D_std_score',\n",
    "                       'card3_card5_DT_W', 'ProductCD_cents_DT_W', 'uid_DT_W', 'D8_DT_W_std_score',\n",
    "                       'ProductCD_TransactionAmt']\n",
    "\n",
    "    \"\"\"\n",
    "     다음 코드는 다음을 수행합니다.\n",
    "     * 학습 및 테스트 데이터에서 특정 열의 값을 결합하고 'value_counts'계산\n",
    "     *`valid_card`를 하나 이상의 값이 있는 카테고리로 정의\n",
    "     * 트레인 데이터의 열 범주도 테스트 데이터에 있으면 유지하고, 그렇지 않으면 None으로 바꿉니다.\n",
    "     * 트레인 데이터의 열 범주가`valid_card`에 있으면 유지하고, 그렇지 않으면 None으로 바꿉니다.\n",
    "     * 테스트 데이터에 대해 동일하게 수행\n",
    "    \"\"\"\n",
    "    for col in ['card1', 'ProductCD_card1', 'card1_addr1', 'TransactionAmt_dist2']:\n",
    "        valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "        valid_card = valid_card[col].value_counts()\n",
    "\n",
    "        valid_card = valid_card[valid_card > 2]\n",
    "        valid_card = list(valid_card.index)\n",
    "\n",
    "        train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "        train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n",
    "\n",
    "        test_df[col] = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)\n",
    "        test_df[col] = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
    "\n",
    "    # 이전과 같이 값이 하나의 데이터 세트에만있는 경우 None으로 바꿉니다\n",
    "    for col in ['card2', 'card3', 'card4', 'card5', 'card6']:\n",
    "        train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "        test_df[col] = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
    "\n",
    "    ####### 투레인 데이터에서 지난달의 최대 값으로 C 열을 자릅니다.\n",
    "    i_cols = ['C' + str(i) for i in range(1, 15)]\n",
    "    for df in [train_df, test_df]:\n",
    "        for col in i_cols:\n",
    "            max_value = train_df[train_df['DT_M'] == train_df['DT_M'].max()][col].max()\n",
    "            df[col] = df[col].clip(None, max_value)\n",
    "\n",
    "    ####### V feature - NaN group agg\n",
    "    # null 값과 열 목록을 포함하는 dictionary가 됩니다.\n",
    "    nans_groups = {}\n",
    "    nans_df = pd.concat([train_df, test_df]).isna()\n",
    "\n",
    "    i_cols = ['V' + str(i) for i in range(1, 340)]\n",
    "    for col in i_cols:\n",
    "        # NaN 값을 세워 봅니다\n",
    "        cur_group = nans_df[col].sum()\n",
    "        if cur_group > 0:\n",
    "            try:\n",
    "                nans_groups[cur_group].append(col)\n",
    "            except:\n",
    "                nans_groups[cur_group] = [col]\n",
    "\n",
    "    for i, (n_group, n_cols) in enumerate(nans_groups.items()):\n",
    "        for df in [train_df, test_df]:\n",
    "            df[f'nan_group_{i}_sum'] = df[n_cols].sum(axis=1)\n",
    "            df[f'nan_group_{i}_mean'] = df[n_cols].mean(axis=1)\n",
    "            df[f'nan_group_{i}_std'] = df[n_cols].std(axis=1)\n",
    "\n",
    "    del nans_groups, nans_df\n",
    "    remove_features += i_cols\n",
    "    # 너무나 많은 공간을 차지합니다. 필요한 것만 남기고 drop 시킵니다.\n",
    "    i_cols = [i for i in i_cols if i not in ['V258', 'V306', 'V307', 'V308', 'V294']]\n",
    "    train_df = train_df.drop(i_cols, axis=1)\n",
    "    test_df = test_df.drop(i_cols, axis=1)\n",
    "\n",
    "    # frequency encoding. 인코딩한 항목과 오리지날 항목을 `remove_features`에 더합니다\n",
    "    i_cols = [\n",
    "        'ProductCD_TransactionAmt', 'ProductCD_cents', 'cents',\n",
    "        'DeviceInfo', 'DeviceInfo_device', 'DeviceInfo_version',\n",
    "        'id_30', 'id_30_device', 'id_30_version',\n",
    "        'id_31', 'id_31_device',\n",
    "        'id_33',\n",
    "    ]\n",
    "    for col in i_cols:\n",
    "        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n",
    "\n",
    "        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n",
    "        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n",
    "\n",
    "        remove_features.append(col)\n",
    "\n",
    "    # 원래 항목을 유지하면서 frequency encoding을 합니다. \n",
    "    i_cols = ['id_01', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_13',\n",
    "              'id_14', 'id_17', 'id_18', 'id_19', 'id_20',\n",
    "              'id_21', 'id_22', 'id_24', 'id_25', 'id_26', 'card1', 'card2', 'card3', 'card5', 'ProductCD_card1',\n",
    "              'card1_addr1', 'TransactionAmt_dist2']\n",
    "    for col in i_cols:\n",
    "        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n",
    "\n",
    "        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n",
    "        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n",
    "\n",
    "    return train_df, test_df, remove_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_val, remove_features = process_data(X_train, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 참조; %%time은 CPU times와 wall time을 출력합니다 %%time은 셀 전체에 대해, %time은 첫 줄에 대한 것만을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "이미 피쳐엔지니어링을 좀 했지만, 기본적인 것 부터 시작하겠습니다.\n",
    "\n",
    "범주 항목에 label encoding부터 하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df_train: pd.DataFrame, df_test: pd.DataFrame, remove_features: list):\n",
    "    train_df = df_train.copy()\n",
    "    test_df = df_test.copy()\n",
    "\n",
    "    # Label Encoding    \n",
    "    for f in train_df.columns:\n",
    "        if train_df[f].dtype == 'object' or test_df[f].dtype == 'object':\n",
    "            train_df[f] = train_df[f].fillna('unseen_before_label')\n",
    "            test_df[f] = test_df[f].fillna('unseen_before_label')\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            train_df[f] = train_df[f].astype('category')\n",
    "            test_df[f] = test_df[f].astype('category')\n",
    "\n",
    "    print('remove_features:', remove_features)\n",
    "\n",
    "    feature_columns = [col for col in list(train_df) if col not in remove_features]\n",
    "    categorical_features = [col for col in feature_columns if train_df[col].dtype.name == 'category']\n",
    "    categorical_features = [col for col in categorical_features if col not in remove_features]\n",
    "\n",
    "    print(f'train.shape : {train_df[feature_columns].shape}, test.shape : {test_df[feature_columns].shape}')\n",
    "\n",
    "    return train_df[feature_columns], test_df[feature_columns], categorical_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, categorical_features = feature_engineering(X_train, X_val, remove_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "모델을 트레인하고 우리의 접근법을 검증할 함수를 만들어 봅니다.\n",
    "\n",
    "요즘 테이블형 데이터는 그래디언트 부스팅 모델로 많이 하는데 우리도 여기서 LightGBM을 사용해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'None',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 2**8,\n",
    "    'max_bin': 255,\n",
    "    'max_depth': -1,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_seed': seed,\n",
    "    'feature_fraction': 0.7,\n",
    "    'feature_fraction_seed': seed,\n",
    "    'first_metric_only': True,\n",
    "    'verbose': 100,\n",
    "    'n_jobs': -1,\n",
    "    'seed': seed,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 competition metric은 roc_auc으로 표현됩니다. \n",
    "\n",
    "기본적인 LightGBM으로는 너무 느릴 수가 잇어서 여기서 우리 나름의 맞춤 평가 함수를 만들어 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def fast_auc(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    nfalse = 0\n",
    "    auc = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n):\n",
    "        y_i = y_true[i]\n",
    "        nfalse += (1 - y_i)\n",
    "        auc += y_i * nfalse\n",
    "    auc /= (nfalse * (n - nfalse))\n",
    "    return auc\n",
    "\n",
    "\n",
    "def eval_auc(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Fast auc eval function for lgb.\n",
    "    \"\"\"\n",
    "    return 'auc', fast_auc(y_true.get_label(), y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_prediction(X_train, y_train, X_val, y_val, seed=0, seed_range=3, lgb_params=None,\n",
    "                        category_cols=None):\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    auc_scores = []\n",
    "    best_iterations = []\n",
    "    val_preds = np.zeros((X_val.shape[0], 3))\n",
    "\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feature_importance_df['feature'] = X_train.columns.tolist()\n",
    "    feature_importance_df['gain_importance'] = 0\n",
    "\n",
    "    for i, s in enumerate(range(seed, seed + seed_range)):\n",
    "        seed_everything(s)\n",
    "        params = lgb_params.copy()\n",
    "        params['seed'] = s\n",
    "        params['bagging_seed'] = s\n",
    "        params['feature_fraction_seed'] = s\n",
    "\n",
    "        clf = lgb.train(params, train_data, 10000, valid_sets=[train_data, val_data],\n",
    "                        categorical_feature=categorical_features,\n",
    "                        early_stopping_rounds=500, feval=eval_auc, verbose_eval=200)\n",
    "\n",
    "        best_iteration = clf.best_iteration\n",
    "        best_iterations.append(best_iteration)\n",
    "        val_pred = clf.predict(X_val, best_iteration)\n",
    "        val_preds[:, i] = val_pred\n",
    "\n",
    "        auc = fast_auc(y_val, val_pred)\n",
    "        auc_scores.append(auc)\n",
    "        print('seed:', s, ', auc:', auc, ', best_iteration:', best_iteration)\n",
    "\n",
    "        feature_importance_df['gain_importance'] += clf.feature_importance('gain') / seed_range\n",
    "\n",
    "    auc_scores = np.array(auc_scores)\n",
    "    best_iterations = np.array(best_iterations)\n",
    "    best_iteration = int(np.mean(best_iterations))\n",
    "\n",
    "    avg_pred_auc = fast_auc(y_val, np.mean(val_preds, axis=1))\n",
    "    print(\n",
    "        f'avg pred auc: {avg_pred_auc:.5f}, avg auc: {np.mean(auc_scores):.5f}+/-{np.std(auc_scores):.5f}, avg best iteration: {best_iteration}')\n",
    "\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='gain_importance', ascending=False).reset_index(\n",
    "        drop=True)\n",
    "    plt.figure(figsize=(16, 12));\n",
    "    sns.barplot(x=\"gain_importance\", y=\"feature\", data=feature_importance_df[:50])\n",
    "    plt.title('LGB Features (avg over folds)');\n",
    "\n",
    "    return feature_importance_df, best_iteration, val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df, best_iteration, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=categorical_features,\n",
    "                                                       lgb_params=lgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making prediction\n",
    "\n",
    "이제 우리는 예측을 만들어 냅니다.\n",
    "\n",
    "StratifiedFold에 대한 전체 데이터에 대한 모델을 학습 할 것입니다.\n",
    "모델에서 유효성 검사를 사용하지 않지만 과적합을 피하기 위해 고정된 수의 반복을 설정합니다.\n",
    "훈련은 폴드에서 수행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, y, X_test, best_iteration, seed=seed, category_cols=None, n_folds=5):\n",
    "    print('best iteration:', best_iteration)\n",
    "    preds = np.zeros((X_test.shape[0], n_folds))\n",
    "\n",
    "    print(X.shape, X_test.shape)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    for i, (trn_idx, _) in enumerate(skf.split(X, y)):\n",
    "        fold = i + 1\n",
    "\n",
    "        tr_x, tr_y = X.iloc[trn_idx, :], y.iloc[trn_idx]\n",
    "\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "\n",
    "        clf = lgb.train(lgb_params, tr_data, best_iteration, categorical_feature=category_cols)\n",
    "        preds[:, i] = clf.predict(X_test)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_full, X_test, remove_features = process_data(X, test.copy())\n",
    "X_full, X_test, categorical_features = feature_engineering(X_full, X_test, remove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = prediction(X_full, y, X_test, best_iteration, category_cols=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['isFraud'] = np.mean(preds, axis=1)\n",
    "sub.to_csv('sub_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More feature engineering\n",
    "\n",
    "이제는 D열을 위주로 작업을 해보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일정 기간 (주 또는 일)이 포함된 함수를 정의하고 이 기간을 기준으로 값을 그룹화한 다음 정규화된 값을 최소 / 최대 및 평균 / 표준으로 계산하는 것으로 만들어 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col + '_' + period\n",
    "\n",
    "            dt_df[col] = dt_df[col].astype(float)\n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n",
    "            temp_mean.index = temp_mean[period].values\n",
    "            temp_mean = temp_mean['mean'].to_dict()\n",
    "\n",
    "            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n",
    "            temp_std.index = temp_std[period].values\n",
    "            temp_std = temp_std['std'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n",
    "            dt_df['temp_std'] = dt_df[period].map(temp_std)\n",
    "\n",
    "            dt_df[new_col + '_min_max'] = (dt_df[col] - dt_df['temp_min']) / (dt_df['temp_max'] - dt_df['temp_min'])\n",
    "            dt_df[new_col + '_std_score'] = (dt_df[col] - dt_df['temp_mean']) / (dt_df['temp_std'])\n",
    "            del dt_df['temp_min'], dt_df['temp_max'], dt_df['temp_mean'], dt_df['temp_std']\n",
    "    return dt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df_train: pd.DataFrame, df_test: pd.DataFrame, remove_features: list):\n",
    "    # 변경되지 않도록 복사합니다\n",
    "    train_df = df_train.copy()\n",
    "    test_df = df_test.copy()\n",
    "\n",
    "    i_cols = ['D' + str(i) for i in range(1, 16)]\n",
    "\n",
    "    ####### Values Normalization\n",
    "    i_cols.remove('D1')\n",
    "    i_cols.remove('D2')\n",
    "    i_cols.remove('D9')\n",
    "    periods = ['DT_D', 'DT_W']\n",
    "    for df in [train_df, test_df]:\n",
    "        df = values_normalization(df, periods, i_cols)\n",
    "\n",
    "    # TransactionAmt Normalization\n",
    "    periods = ['DT_D', 'DT_W']\n",
    "    for df in [train_df, test_df]:\n",
    "        df = values_normalization(df, periods, ['TransactionAmt'])\n",
    "\n",
    "    # normalize by max train value\n",
    "    for col in ['D1', 'D2']:\n",
    "        for df in [train_df, test_df]:\n",
    "            df[col + '_scaled'] = df[col] / train_df[col].max()\n",
    "\n",
    "    # frequency encoding\n",
    "    i_cols = ['D' + str(i) for i in range(1, 16)] + ['DT_D', 'DT_W', 'ProductCD_TransactionAmt', 'ProductCD_cents']\n",
    "    for col in i_cols:\n",
    "        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n",
    "\n",
    "        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n",
    "        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n",
    "\n",
    "        remove_features.append(col)\n",
    "\n",
    "    # Label Encoding    \n",
    "    for f in train_df.columns:\n",
    "        if train_df[f].dtype == 'object' or test_df[f].dtype == 'object':\n",
    "            train_df[f] = train_df[f].fillna('unseen_before_label')\n",
    "            test_df[f] = test_df[f].fillna('unseen_before_label')\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            train_df[f] = train_df[f].astype('category')\n",
    "            test_df[f] = test_df[f].astype('category')\n",
    "\n",
    "    print('remove_features:', remove_features)\n",
    "    print(f'train.shape : {train_df.shape}, test.shape : {test_df.shape}')\n",
    "\n",
    "    ########################### Final features list\n",
    "    feature_columns = [col for col in list(train_df) if col not in remove_features]\n",
    "    print('feature_columns:', len(feature_columns))\n",
    "    categorical_features = [col for col in feature_columns if train_df[col].dtype.name == 'category']\n",
    "    categorical_features = [col for col in categorical_features if col not in remove_features]\n",
    "\n",
    "    return train_df[feature_columns], test_df[feature_columns], categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, remove_features = process_data(X_train, X_val)\n",
    "X_train, X_val, categorical_features = feature_engineering(X_train, X_val, remove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df, best_iteration, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=categorical_features,\n",
    "                                                       lgb_params=lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_full, X_test, remove_features = process_data(X, test.copy())\n",
    "X_full, X_test, categorical_features = feature_engineering(X_full, X_test, remove_features)\n",
    "preds = prediction(X_full, y, X_test, best_iteration, category_cols=categorical_features)\n",
    "sub['isFraud'] = np.mean(preds, axis=1)\n",
    "sub.to_csv('sub_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More feature engineering 2\n",
    "\n",
    "이제는 C-columns에 대한 피쳐 엔지니어링을 좀 해 보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df_train: pd.DataFrame, df_test: pd.DataFrame, remove_features: list):\n",
    "    # 원본이 변경되지 않도록 복사해 사용합니다\n",
    "    train_df = df_train.copy()\n",
    "    test_df = df_test.copy()\n",
    "\n",
    "    i_cols = ['D' + str(i) for i in range(1, 16)]\n",
    "\n",
    "    ####### Values Normalization\n",
    "    i_cols.remove('D1')\n",
    "    i_cols.remove('D2')\n",
    "    i_cols.remove('D9')\n",
    "    periods = ['DT_D', 'DT_W']\n",
    "    for df in [train_df, test_df]:\n",
    "        df = values_normalization(df, periods, i_cols)\n",
    "\n",
    "    # TransactionAmt Normalization\n",
    "    periods = ['DT_D', 'DT_W']\n",
    "    for df in [train_df, test_df]:\n",
    "        df = values_normalization(df, periods, ['TransactionAmt'])\n",
    "\n",
    "    # normalize by max train value\n",
    "    for col in ['D1', 'D2']:\n",
    "        for df in [train_df, test_df]:\n",
    "            df[col + '_scaled'] = df[col] / train_df[col].max()\n",
    "\n",
    "    # frequency encoding\n",
    "    i_cols = ['D' + str(i) for i in range(1, 16)] + ['DT_D', 'DT_W', 'ProductCD_TransactionAmt', 'ProductCD_cents']\n",
    "    for col in i_cols:\n",
    "        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n",
    "\n",
    "        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n",
    "        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n",
    "\n",
    "        remove_features.append(col)\n",
    "\n",
    "    i_cols = ['C' + str(i) for i in range(1, 15)]\n",
    "    # 여기서는 C column의 0들을 카운트합니다\n",
    "    for df in [train_df, test_df]:\n",
    "        df['c_cols_0_bin'] = ''\n",
    "        for c in i_cols:\n",
    "            df['c_cols_0_bin'] += (df[c] == 0).astype(int).astype(str)\n",
    "    col_name, freq_dict = freq_encode_full(train_df, test_df, 'c_cols_0_bin')\n",
    "\n",
    "    train_df[col_name] = train_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n",
    "    test_df[col_name] = test_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n",
    "\n",
    "    for col in i_cols:\n",
    "        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n",
    "\n",
    "        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n",
    "        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n",
    "\n",
    "    # Label Encoding    \n",
    "    for f in train_df.columns:\n",
    "        if train_df[f].dtype == 'object' or test_df[f].dtype == 'object':\n",
    "            train_df[f] = train_df[f].fillna('unseen_before_label')\n",
    "            test_df[f] = test_df[f].fillna('unseen_before_label')\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            train_df[f] = train_df[f].astype('category')\n",
    "            test_df[f] = test_df[f].astype('category')\n",
    "\n",
    "    print('remove_features:', remove_features)\n",
    "    print(f'train.shape : {train_df.shape}, test.shape : {test_df.shape}')\n",
    "\n",
    "    ########################### Final features list\n",
    "    feature_columns = [col for col in list(train_df) if col not in remove_features]\n",
    "    print('feature_columns:', len(feature_columns))\n",
    "    categorical_features = [col for col in feature_columns if train_df[col].dtype.name == 'category']\n",
    "    categorical_features = [col for col in categorical_features if col not in remove_features]\n",
    "\n",
    "    return train_df[feature_columns], test_df[feature_columns], categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, remove_features = process_data(X_train, X_val)\n",
    "X_train, X_val, categorical_features = feature_engineering(X_train, X_val, remove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df, best_iteration, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=categorical_features,\n",
    "                                                       lgb_params=lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_full, X_test, remove_features = process_data(X, test.copy())\n",
    "X_full, X_test, categorical_features = feature_engineering(X_full, X_test, remove_features)\n",
    "preds = prediction(X_full, y, X_test, best_iteration, category_cols=categorical_features)\n",
    "sub['isFraud'] = np.mean(preds, axis=1)\n",
    "sub.to_csv('sub_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More feature engineering - aggregations\n",
    "\n",
    "데이터에 대한 다양한 집계를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uid_aggregation(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:\n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col + '_' + main_column + '_' + agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col, main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                    columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()\n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name] = test_df[col].map(temp_df)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df_train: pd.DataFrame, df_test: pd.DataFrame, remove_features: list):\n",
    "    # 복사해 둡니다\n",
    "    train_df = df_train.copy()\n",
    "    test_df = df_test.copy()\n",
    "\n",
    "    i_cols = ['D' + str(i) for i in range(1, 16)]\n",
    "\n",
    "    ####### Values Normalization\n",
    "    i_cols.remove('D1')\n",
    "    i_cols.remove('D2')\n",
    "    i_cols.remove('D9')\n",
    "    periods = ['DT_D', 'DT_W']\n",
    "    for df in [train_df, test_df]:\n",
    "        df = values_normalization(df, periods, i_cols)\n",
    "\n",
    "    # TransactionAmt Normalization\n",
    "    periods = ['DT_D', 'DT_W']\n",
    "    for df in [train_df, test_df]:\n",
    "        df = values_normalization(df, periods, ['TransactionAmt'])\n",
    "\n",
    "    for col in ['D1', 'D2']:\n",
    "        for df in [train_df, test_df]:\n",
    "            df[col + '_scaled'] = df[col] / train_df[col].max()\n",
    "\n",
    "    i_cols = ['D' + str(i) for i in range(1, 16)] + ['DT_D', 'DT_W', 'ProductCD_TransactionAmt', 'ProductCD_cents']\n",
    "    for col in i_cols:\n",
    "        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n",
    "\n",
    "        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n",
    "        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n",
    "\n",
    "        remove_features.append(col)\n",
    "\n",
    "    i_cols = ['C' + str(i) for i in range(1, 15)]\n",
    "\n",
    "    for df in [train_df, test_df]:\n",
    "        df['c_cols_0_bin'] = ''\n",
    "        for c in i_cols:\n",
    "            df['c_cols_0_bin'] += (df[c] == 0).astype(int).astype(str)\n",
    "    col_name, freq_dict = freq_encode_full(train_df, test_df, 'c_cols_0_bin')\n",
    "\n",
    "    train_df[col_name] = train_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n",
    "    test_df[col_name] = test_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n",
    "\n",
    "    for col in i_cols:\n",
    "        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n",
    "\n",
    "        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n",
    "        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n",
    "\n",
    "    i_cols = ['TransactionAmt', 'id_01', 'id_02', 'id_05', 'id_06', 'id_09', 'id_14', 'dist1'] + ['C' + str(i) for i in\n",
    "                                                                                                  range(1, 15)]\n",
    "    uids = ['card1', 'card2', 'card3', 'card5', 'uid', 'card3_card5']\n",
    "    aggregations = ['mean', 'std']\n",
    "\n",
    "    # uIDs aggregations\n",
    "    train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    "\n",
    "    i_cols = [\n",
    "                 'V258',\n",
    "                 'V306', 'V307', 'V308', 'V294'\n",
    "             ] + ['D' + str(i) for i in range(1, 16)]\n",
    "    uids = ['uid', 'card3_card5']\n",
    "    aggregations = ['mean', 'std']\n",
    "    train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    "\n",
    "    # Label Encoding    \n",
    "    for f in train_df.columns:\n",
    "        if train_df[f].dtype == 'object' or test_df[f].dtype == 'object':\n",
    "            train_df[f] = train_df[f].fillna('unseen_before_label')\n",
    "            test_df[f] = test_df[f].fillna('unseen_before_label')\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            train_df[f] = train_df[f].astype('category')\n",
    "            test_df[f] = test_df[f].astype('category')\n",
    "\n",
    "    print('remove_features:', remove_features)\n",
    "    print(f'train.shape : {train_df.shape}, test.shape : {test_df.shape}')\n",
    "\n",
    "    ########################### Final features list\n",
    "    feature_columns = [col for col in list(train_df) if col not in remove_features]\n",
    "    print('feature_columns:', len(feature_columns))\n",
    "    categorical_features = [col for col in feature_columns if train_df[col].dtype.name == 'category']\n",
    "    categorical_features = [col for col in categorical_features if col not in remove_features]\n",
    "\n",
    "    return train_df[feature_columns], test_df[feature_columns], categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, remove_features = process_data(X_train, X_val)\n",
    "X_train, X_val, categorical_features = feature_engineering(X_train, X_val, remove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df, best_iteration, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=categorical_features,\n",
    "                                                       lgb_params=lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_full, X_test, remove_features = process_data(X, test.copy())\n",
    "X_full, X_test, categorical_features = feature_engineering(X_full, X_test, remove_features)\n",
    "preds = prediction(X_full, y, X_test, best_iteration, category_cols=categorical_features)\n",
    "sub['isFraud'] = np.mean(preds, axis=1)\n",
    "sub.to_csv('sub_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More feature engineering - time-block frequency encoding\n",
    "\n",
    "이것은 일반적인 frequency 인코딩과 유사하지만 해당 기간 내에 값을 인코딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeblock_frequency_encoding(train_df, test_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col + '_' + period\n",
    "            print('timeblock frequency encoding:', new_col)\n",
    "            train_df[new_col] = train_df[col].astype(str) + '_' + train_df[period].astype(str)\n",
    "            test_df[new_col] = test_df[col].astype(str) + '_' + test_df[period].astype(str)\n",
    "\n",
    "            temp_df = pd.concat([train_df[[new_col]], test_df[[new_col]]])\n",
    "            fq_encode = temp_df[new_col].value_counts(normalize=True).to_dict()\n",
    "\n",
    "            train_df[new_col] = train_df[new_col].map(fq_encode)\n",
    "            test_df[new_col] = test_df[new_col].map(fq_encode)\n",
    "\n",
    "            train_df[new_col] = train_df[new_col] / train_df[period + '_freq_enc_full']\n",
    "            test_df[new_col] = test_df[new_col] / test_df[period + '_freq_enc_full']\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df_train: pd.DataFrame, df_test: pd.DataFrame, remove_features: list):\n",
    "    # 복사해 둡니다\n",
    "    train_df = df_train.copy()\n",
    "    test_df = df_test.copy()\n",
    "\n",
    "    i_cols = ['D' + str(i) for i in range(1, 16)]\n",
    "\n",
    "    ####### Values Normalization\n",
    "    i_cols.remove('D1')\n",
    "    i_cols.remove('D2')\n",
    "    i_cols.remove('D9')\n",
    "    periods = ['DT_D', 'DT_W']\n",
    "    for df in [train_df, test_df]:\n",
    "        df = values_normalization(df, periods, i_cols)\n",
    "\n",
    "    # TransactionAmt Normalization\n",
    "    periods = ['DT_D', 'DT_W']\n",
    "    for df in [train_df, test_df]:\n",
    "        df = values_normalization(df, periods, ['TransactionAmt'])\n",
    "\n",
    "    for col in ['D1', 'D2']:\n",
    "        for df in [train_df, test_df]:\n",
    "            df[col + '_scaled'] = df[col] / train_df[col].max()\n",
    "\n",
    "    i_cols = ['D' + str(i) for i in range(1, 16)] + ['DT_D', 'DT_W', 'ProductCD_TransactionAmt', 'ProductCD_cents']\n",
    "    for col in i_cols:\n",
    "        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n",
    "\n",
    "        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n",
    "        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n",
    "\n",
    "        remove_features.append(col)\n",
    "\n",
    "    i_cols = ['C' + str(i) for i in range(1, 15)]\n",
    "\n",
    "    for df in [train_df, test_df]:\n",
    "        df['c_cols_0_bin'] = ''\n",
    "        for c in i_cols:\n",
    "            df['c_cols_0_bin'] += (df[c] == 0).astype(int).astype(str)\n",
    "    col_name, freq_dict = freq_encode_full(train_df, test_df, 'c_cols_0_bin')\n",
    "\n",
    "    train_df[col_name] = train_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n",
    "    test_df[col_name] = test_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n",
    "\n",
    "    for col in i_cols:\n",
    "        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n",
    "\n",
    "        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n",
    "        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n",
    "\n",
    "    i_cols = ['TransactionAmt', 'id_01', 'id_02', 'id_05', 'id_06', 'id_09', 'id_14', 'dist1'] + ['C' + str(i) for i in\n",
    "                                                                                                  range(1, 15)]\n",
    "    uids = ['card1', 'card2', 'card3', 'card5', 'uid', 'card3_card5']\n",
    "    aggregations = ['mean', 'std']\n",
    "\n",
    "    # uIDs aggregations\n",
    "    train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    "\n",
    "    i_cols = [\n",
    "                 'V258',\n",
    "                 'V306', 'V307', 'V308', 'V294'\n",
    "             ] + ['D' + str(i) for i in range(1, 16)]\n",
    "    uids = ['uid', 'card3_card5']\n",
    "    aggregations = ['mean', 'std']\n",
    "    train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    "\n",
    "    i_cols = ['ProductCD_TransactionAmt', 'ProductCD_cents', 'ProductCD_cents', 'uid', 'card3_card5']\n",
    "    periods = ['DT_D', 'DT_W']\n",
    "    train_df, test_df = timeblock_frequency_encoding(train_df, test_df, periods, i_cols)\n",
    "\n",
    "    # Label Encoding    \n",
    "    for f in train_df.columns:\n",
    "        if train_df[f].dtype == 'object' or test_df[f].dtype == 'object':\n",
    "            train_df[f] = train_df[f].fillna('unseen_before_label')\n",
    "            test_df[f] = test_df[f].fillna('unseen_before_label')\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            train_df[f] = train_df[f].astype('category')\n",
    "            test_df[f] = test_df[f].astype('category')\n",
    "\n",
    "    print('remove_features:', remove_features)\n",
    "    print(f'train.shape : {train_df.shape}, test.shape : {test_df.shape}')\n",
    "\n",
    "    ########################### Final features list\n",
    "    feature_columns = [col for col in list(train_df) if col not in remove_features]\n",
    "    print('feature_columns:', len(feature_columns))\n",
    "    categorical_features = [col for col in feature_columns if train_df[col].dtype.name == 'category']\n",
    "    categorical_features = [col for col in categorical_features if col not in remove_features]\n",
    "\n",
    "    return train_df[feature_columns], test_df[feature_columns], categorical_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, remove_features = process_data(X_train, X_val)\n",
    "X_train, X_val, categorical_features = feature_engineering(X_train, X_val, remove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df, best_iteration, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=categorical_features,\n",
    "                                                       lgb_params=lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_full, X_test, remove_features = process_data(X, test.copy())\n",
    "X_full, X_test, categorical_features = feature_engineering(X_full, X_test, remove_features)\n",
    "preds = prediction(X_full, y, X_test, best_iteration, category_cols=categorical_features)\n",
    "sub['isFraud'] = np.mean(preds, axis=1)\n",
    "sub.to_csv('sub_5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
